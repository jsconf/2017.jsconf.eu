"Dynamic video in the browser"
Matthew Shotton

MATTHEW:  Today I'm going to talk to you about doing dynamic video in the browser.  
    
    [ Lapse in Audio ]
    
    I want to give you a brief history of broadcast.  In the beginning, people transferred not news -- as if information would change and adapt over time.  Then, the written word came to be locked down to a single representation, and this was almost the first form of broadcasting.  Then radio -- 
    
    [ Lapse in Audio ]
    
    -- information could be delivered to millions of people live.  And that was shortly followed by -- and then -- you had a network of communication that had the potential to let -- and that was amazing.  Good people worked really, really hard -- the radio --because I really love making stuff.
    But I'm going to talk -- 
    
    [ Lapse in Audio ]
    
    -- called the video context.  Context is -- and its aim is to make it really easy to make interactive video on the web.  The Web Audio API -- so, like, the video context is very similar to the Web Audio API, but for video.  HTML media elements like videos, images and -- 
    
    [ Lapse in Audio ]
    
    -- so all image processing operations are done in shaders -- and a really simple revealed context operation.  This is a HTML document and the video context is brought in the at the top and we have a canvas, and that's where we're going to render to.  So I'm going to zoom in.
    So we first get a reference to the canvas and then we pass it to a new video context instance and let me create some image source nodes.  So source nodes in the video context --
    
    [ Audio Dropped ]
    
    There's a code that runs off the GPU.  And they typically one in a C-like -- which gets run in parallel across under processing units on the GPU.  And this makes it easy to do simple operations cause many, many -- and this describes what that effect should do.  We then disconnect the video node from the destination, and then the color threshold node, and then we connect the color threshold node to the destination.  Quickly, I'll give you a brief look at what those effect definitions look like.  It's mostly shader code.  So the bit in the box is the fragment shader code that does the green-screening.  So the video context has a bunch of effects built in but it's also quite a nice environment for building and experimenting with your own shaders.  So back to the code.  We've created a digraph which looks like this.  We have a video node which is connected to the effect node, which is then connected to the destination, and that results in this, which is much better.  We can see our space cats again.
    
    [ Applause ]
    
    Oh, thank you!  Yeah, so we're going to, again, take you through that loop one more time.  We're going to introduce transitions because transitions are really important in video editing.  So a transition is how you might move from a video to the next.  So instead of a straight cut, you might have something like a starlight, or something jazzy.  And so we're going to create a new video node and this is created using the same source as the -- that tells the video context how far into that source video to seek before it starts playing back so this is how you can sort of cut up pre-existing video files by using this library.  So we then create a transition node.  And this is very similar to the effect node except we pass in a transition definition.  And then we call transition on it.
    So this is going to transition from 0.3 to 0.6, and then we connect the two videos to the transition node and the transition node to the color refresher node and then we set the second video node to start playing.  And this produces a graph that looks like this.  And this is getting a bit complicated now and it might be a bit hard to read.  But it results in this.
    So we've got Shia LaBeouf with the green screen in between, but they're being transitioned in between.  But, wait, there's more.  So as I said, the video context has a bunch of effects built in and a bunch of effects built in.  And this is a demonstration I've put together to kind of push the limits.  So this is 11 simultaneous effects and five transition effects happening in real time on full-frame images.  And this was -- yeah, rendered live in the browser.  And then each one was scaled to fit into this grid, and this really shows the power of shaders, that you're able to do this much processing, this quickly in the browser.  It's crazy.
    So other features.  Everything I've shown you about the video context so far has been very much set up our processing graphs, set up our running times, and say we're running.  But it allows you to do everything dynamically, as well.  So at any point I could have connected, or disconnected those nodes, or at any point, I could have reconnected those effects.  It also has limited mobile support which I'll go into in a little bit, and if you're interested in integrating with video it does that, as well.  So now I'll talk about the challenges that I came across, and these challenges are pretty agnostic, and they'll likely come up if you're trying to build interactive video experiences yourself.
    So most browsers are limited to about six to eight simultaneous HTTP connections per domain and if you have many clips that you want to play back-to-back, the naïve way of doing this, is to create a HTML element for each of those clips, but this quickly saturates the number of requests, and it can make where you are website really slow and unresponsive.  So the solution to this is all HTML elements are about four seconds than you actually need them which gives you any more time to get them preloaded and working.  So there sort of needs to be a master clock to sync everything to in the master context and the natural fit for this would seem to be the video.currentTime attribute.  And unfortunately, this varies between browsers, so if you're doing things that require frame-accurate cutting between clips, if your current time attribute is only updating every fifth of a second, it can break things.  So the solution to this is to use a requestAnimationFrame, this doesn't come without its own problems.  So if you switch tabs, the requestAnimationFrame call gets halted.  So this means, traditionally on the web, if you're playing a video and you switch tabs, you would expect the video to play in the background.  But it won't be able to do that.  So the solution is as soon as you -- inside the web worker run the setTimeout loop.  It will give you enough time to keep the video context ticking over.  Okay.  I'm a little bit ashamed of this one, so bear with me.
    So on mobile, there's no auto playing video, and this is to prevent audio files from playing and using up your mobile data.  So the reason for this is all control calls to a video element must originate from -- or a first one must originate from a first element for a video element to be controlled programmatically on mobile.  So the solution to this is we create a pool of elements with no source, and -- excuse me -- with no source and when you call play for the first time in a video context, play gets called on all of those video elements but nothing happens because there's no source attribute but it puts them into this activated state where they can then control them programmatically.  So then the video context manages this pool of the video elements, it will use it to play my video and then it would put it back into the pool.  So this allows you to play back video on mobile that doesn't start at the very beginning, but sequences.
    So finally, this is kind of a biggie.  There's no lower-level API for frame synchronization across multiple videos and this is kind of a problem because we have quite complex timing requirements.  We have videos playing on a timeline at any point.  We also have offsets within those videos if we're taking a clip all of them.  We need to make sure they all stay in-sync.  The solution to this one, you kind of -- you gotta set them playing and then hope for the best, basically.  You can do some quite naïve things.
    So if the video starts buffering, then pause one of the videos and wait for it to start playing again.  And it turns out, perceptually, it isn't too bad.  People have an expectation on video that it will buffer now and then.  Which, if broadcasting if it happens, that's a massive no-no, but you can get away with it on the web a bit more.  So this is the question:  Why are we doing this?  It's all well and fun.  As much as some of us might want to turn broadcasting into cat gifs and Shia LaBeouf, that's probably not realistic.  And this goes the BBC's mission statement.  And it didn't talk about the broadcasting which possibly eliminates the crisis.  So in R&D where I work, we use new technologies to inform people for ways that are faster, and specifically, the user experience team where I work, we use user-led design to drive novel experiences to drive change.  And the tool for this is generally the web and so I'll show you a quick demo video and this is one of the prototypes that we've built with this and we've built a whole range of these, but I'll just show you this one.
    >> Forecaster from the BBC Labs shows how it can deliver more flexible experiences for audiences.  In this demonstrator, we transmit even of the needed elements separately and because of this, we have the ability to control each of these individual elements in isolation from one another.  The timeline for when you actually see the bottom of the footage represents the timeline for each media object in the forecast.  To illustrate this, you can see how the onscreen graphics can be turned on or off.  Or the whole backdrop removed to reveal the raw greenscreen footage.  By delivering content in this way, one of the biggest potentials lies in the enhancement of -- with a listener is a first-class -- in the video for example.  Or when subtitle elements are present to avoid overlap.  You can also change the background map to a higher contrast view to aid the visually-impaired.
    If media were delivered in this way, we could also have the footage adapting to suit your screen size, rather than forcing a single aspect ratio or font size on all devices.
    Here, we can see how a mobile portrait view would look with a larger font size and a rearranged view to include more of the map.  In the future, we could also link content to your personal calendar, or an other third-party data services and feature relevant information.  As you can see, the ability to send media elements, or objects separately rather than as a single video stream gives much greater flexibility for playback, allowing content to automatically adapt to the screen size and preferences of the viewer.
    The flexibility of an object-based approach won't just benefit audiences, though.  BBC R&D is also testing how it can make production more efficient giving program makers time to flex their creative muscles in new ways.  We hope you've enjoyed this sneak peek into the IP production in the future.  This does not represent a new service, and uses non-broadcast maps only.
    MATTHEW:  I love that disclaimer at the end.  Thank you.  So, um, in that, you heard the phrase object-based media and this is what R&D are calling this new approach to broadcasting and it might feel quite familiar if you've been on the web for a while but we like to think of it as bringing responsive design, making content that adapts to the user, and to the device, and to the environment they're in.  And those were one of the things that we were exploring in building this area up a bit more.
    So finally, as well as a new way to deliver content, the web is a new medium for storytelling in and of itself.  And we're really excited to be exploring this area, we hope that by releasing this library, we'll be make this easier for other people, as well, and we're super excited to see what you'll make with it.  Thanks.
    
    [ Applause ]
    
    I think we should have another round of applause because the first one's pretty weak and that was an awesome talk.  Come on again!
    
    [ Applause ]
    
    Really, everyone needs more coffee.  I feel like no one's anxious enough, and, like, pumped enough.  We'll have another few minutes to let people come from the other track as we are ahead of time.  There'll be no dancing unless someone wants to come in and dance who's better than me, that's great.  I'm not going to do that.
    
    [ Short Break ]