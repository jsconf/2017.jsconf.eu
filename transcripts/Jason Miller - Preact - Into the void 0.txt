Jason Miller - Preact - Into the void 0
>> I was really hoping for the music to keep working so I cannot entertain everyone, but, I guess, now I have to, so that's really awkward. Are you ready, Berlin?  
AUDIENCE:  Wooh!  
>> I'm personally really, really excited about this upcoming block of talks here at the Back Track because we are going to be talking about the state of connectivity, we are going to be talking about performance and how to make the web faster for everyone. The first speaker, Jason from Canada will be talking about React - which he wrote and how awesome is that - a super lightweight performance. Are you ready, Jason?  
JASON:  No?  
>> Does one want to do interpretative dance?  And there is no mucic so that's really awkward. [Music begins]. Thanks to Malta for amazing tunes. He is the director of music and visuals and JSConf. All the music has been really amazing, thanks to him. He wears the best socks of JSConf and best T-shirts generally. A two-laptop situation. Is it getting serious?  
JASON:  Close enough. 3D rendering. 
>> Give it up to Jason!  [Applause].  
JASON:  Hi, everybody.  I'm Jason, as was advertised before when the slides came up. I'm a developer.  I'm a serial library author, as some of you may know - that's self-diagnosed. I like donuts and poutine and axes which means I'm Canadian. I like constraints. I've been developing for the mobile web before the mobile web was something anybody would wilfully use. I've written a bunch of UI frameworks. It is a serious problem I've developed that I'm trying to work around, and the reason why is I find constraints to be interesting challenges. I have something called ADHD - you're probably familiar with that - it basically means that, in order to do something effectively, it is best if I can hyperfocus on it and the interesting constraint challenges provide me a situation where it is true. I wrote Preact and called into the void zero. That's the only semicolon you will see in these slides!  You might be wondering what is Preact. This is it in its entirety. I did remove the source map comment. Today we're going to be to touch that on and that. Mostly the second one. We can't even talk about them before we even talk about this stuff up here, and this is a Preact presentation, so naturally, we need something in purple, so, whatever that is, that will be the last thing we touch on. Before we did get to any of this, we need to talk about jsx. If you're not familiar with it, I don't know what ruck you've been living under. Don't worry, it is really, really easy to understand. Jsx is simple at its core. By the end of my talk, you will be starting to rewrite your webpack configuration in jsx which makes is it longer and complex which is a good thing. If you don't believe me, go and look at the webpack 2docs and that is in them. What is JSX?  It's an XML-like expression compiled to a function call. We write the thing that you see on the left - we write in these HTML-looking angle-bracket syntaxes and on the right is what we get out of the transpiler like Babel or Bublé  - there are ten transpilers now. The whole point to me of JSX is I would be happy writing the thing on the right. This is verified by the fact that I'm constantly Tweeted at by people who much prefer the thing on the right. I think telling. I like JSX because I like parity with the DOM. The thing on the right is something people are happy to write. It is important this is a transparent thing sitting in front of an understandable thing. There is one little complexity on top of this from JSX and it is you can get back into JavaScript from these angle-bracket syntaxes. You can see on the left one and the variable "world" are on the right. If you use the capital letter as the first letter of a tag name, resulted in a generating code. So, the essence of JSX is this factory function we saw. It is really simple. It just has a signature that accepts the things that you ... tag name, the same thing, which will be a string or function. Really, it could be anything. It's going to be one of these two. You have attributes which could be optional, an object, and the rest of the arguments are children, and this is how we do composition. You might be thinking to yourself, aren't you just writing hyperscript, right?  Isn't really what this is under the hood?  You wouldn't be wrong. Hyperscript is very similar in, in some cases, it is a slight super set of JSX because of this. This!  So hyperscript supports this additional tag syntax where you can use essentially CSS selectors to predefined attributes on the element and JSX does not have that. So, what JSX does have is the ability to use these sort of extended tag names, so, it is important that we have a solid understanding of JSX because the concept is simple and direct, but it is the interface through which we access all of these virtual DOM libraries. Please remember that JSX is not DOM, not related to DOM, it is just a syntax. It doesn't know anything about your code or how it is intended to work. You can use this to write your webpack configuration. Please don't. You could use it to write XML, so if you want to write a soap client and you want to use parsing and serialisation, you could use JSX for that. I would recommend that as an interesting solution to a problem. That's JSX on the way. The next step in our adventure is virtual DOM. Virtual DOM are objects that represent a tree structure. That's it. There's nothing else going on. I like to think of it as configuration that you're passing to a DOM builder of some kind and we are going to walk through in making the DOM builder not so theoretical. The first thing we need to understand how do we get from JSX to V-DOM. It is fairly straightforward. We know that we wrote our JSX, we got our H function calls. We need to have an H define such that it produces these objects. This is Virtual Dom. Sometimes, it gets trumped up beyond this. This is what is it looks like. It's just nested objects. This is surprisingly easy to do. This is how you do it. It is a one-line function. You could normalise children, remove empty values, coalesce adjacent string nodes - lots of stuff - but at its volunteer, you could write a virtual DOM renderer using this function. Let's do that and write a virtual DOM renderer. We will pass ourselves a V node. This is the object we saw up there on the right of the scroll bar. So the first thing is, we need to create a DOM element that matches the type of the V DOM element so we can use that using "create element" pretty straightforward. One ID equals foo and then set up recursion so we're going to loop over children, pass them back to render and take the resulting DOM element and append it to our current DOM element. There is one slight complexity here you might have noticed:  one of the children is a string. One of these things is not like the other. We need to deal with that hello text. The way we do that, if it is a string, we bail out with a text note. That's our virtual DOM renderer. There is one slight complexity here in attributes - notice I've called them attributes. If anyone has used React, you're probably getting hot under the collar about that. They're not attributes. Don't let anyone tell you that except me. I don't know why I've chosen why I can get away with it - forth duration of the talk I can!  They're props - they're an abstraction between these two things. Most HTML elements can accept data as attributes, and they can also accept similar, often typed data, as properties through something called DOM property reflection. But both of these are technically wrong. There are things we can do with attributes that we can't do with properties and things we can't do with properties that we can do with attributes. What we need is a way to have both. Thinking about Preact, with you can't use whitelists. Self-closing tag names, that's not a thing that Preact could never do. Instead, we have a reference to a DOM node and we ask the DOM node, "Do you have a support for a property foo"?  If it does, we use the property, if not, we fall back on the attribute. It is good for custom elements, and we use those and move forward. So the question you may be asking yourself is, "Does it work?"  Sorry, I have a really bad Sean Connery impression. I promised someone I wouldn't do that and I had to do it in order to break the promise!  Here's the virtual DOM we wrote. We will pass the through the render function we note. Here is the output in JSFiddle and it works. Deploy to production. Be thankful you don't work with me!  We build a simple virtual DOM renderer. It is a terrible virtual DOM renderer, it's a version 0, but it is terrible because it doesn't diff. It doesn't look at the current state of the DOM and update the DOM to reflect anything. It just blows away the whole DOM and replaces it with something. The debate is warranted because there are trade-offs here that you can't treat as black and white things. Diffing is basically this idea of instead of rendering top down and creating a new as we rendered top down, we're going to pass ourselves a reference to what the DOM looks like now and then mutate it to look like what this JSX-derived looks like, our virtual DOM, just apply the differences. On the left, you can see what the virtual DOM looks like. On the right, you can see what the actual DOM looks like. You can see that the nails are the name or similar. There is actually a DOM children property, it just include text nodes so we don't use that. You can see how you can run through this and perform a comparison left to right and apply the changes on the right. So, it's basically a three-step process to run through the diff. First step is type. Before we can do anything, we need to create an element with the correct node name. This is pretty straightforward, but that's kind of step one here. The second step is we are going to loop over the children, perform a by-directional comparison and figure out whether we need to add, remove children, rearrange them, et cetera. The last step is to update the attribute/props with the new values from our virtual DOM tree. We set up incursion by looping over the children. Let's start with type. We enter your our diff. Is this node owned by a component?  If it is not owned by a component, things are really easy. We look at the node type - in this case, node name - if it is the same, we will update it in place and render it. If it is different, we're going to throw out the node and create a new one and then continue diffing down the tree, setting attributes and trees and so on. If a node was owned by a component and things are a little bit more complex, basically we needed to create a backing instance. Create it if it doesn't exist and update it if it does. Then things get easy, we call rending on that backing instance which we pass back into the same diff function so it's not that complex. There are intricacies here with life cycle, but as far as a chart on a slide goes, it is pretty simple. So, children is even easier than that. It is basically three steps. The first step is we loop all over the existing children in the DOM and put them in the lists. There is an unkeyed list which is ordered and a keyed list which is named. The second step is we move over the new virtual children which are these lightweight objects or strings and we find the first match in our list of unkeyed or our map of keyed, and we find that or null, an element, and diff that against our virtual DOM child objection. We have a specification for that and we insert that in the current index we're at in the virtual loop. The last stuff is the easiest:  if there are any kids left, we delete them because they're not used in the updated definition of what the DOM should look like. You might be wondering about the keyed versus unkeyed lists. This is probably the worst understood of virtual DOMs. I tried to explain this on Stack Overflow and butchered it. Keys attribute meaningful order to virtual DOM elements when they have uniform type. So we can see in in example, we have three list its - one, two, and three. In the second render here, we only have two list items, and it is clear to you and I as humans that we've deleted item 2, that the text 2 is gone so we know logically, we deleted the second item and the third one has moved up. A virtual DOM renderer does not know that. There is nothing saying that item 2 is item 2 in the second render. It receives a new tree each time. There's no way for it to correlate these things. So what it is going to do is going to look at one - one is unchanged. That's easy. Nothing happens. It is going to look at 2 and it says, "Oh, no, they changed the context of 2 to 3. That's fine, I will update that." It gets to three and it's gone in the second render. It will kill that node. The thing to understand here is by default for a list of uniform elements in virtual DOM it will always push off and pop at the end. It has no concept of removing and adding to the middle of the child list. Contrast this with a keyed approach. In a keyed approach, we give each element a unique key - they do have to be unique. In the first render, we've got one, two, and three. In the second render, one and three. It's obvious the element with key 2 has been removed. Now we tell the DOM renderer what to do, so it knows when it loops over the list, it will delete it and move to item 3 which will move up in its place. It makes things really, really simple for the renderer. It is kind of cheating. You're essentially telling the diff to do its own job, but in the where you would like to control the diff, it is nice to have the escape hatch. The last step for the diff process is attributes. This is really easy. We give ourselves the old properties in the new properties. We loop over the old ones, find the ones that are not in the new property bag and set them to undefine in the DOM and then we do the exact opposite. For each new attribute, we check if it is new values, check if it was defined and now undefined, it is going undefined to some value and then we set its value in the DOM to the corresponding property. So we solved all of life's problems and now our app is as fast as it could possibly, except this is not the case. All we've done is move our problems into a library, in Preact, React, Inferno - the list goes on. I want to tell you guys a little bit about the performance journey I've been on in writing Preact, trying to benchmark it and take it further than it currently is. This is something you will often see people say, especially if you hang out on Twitter for various reasons. We often hear complaints the DOM is slow, the DOM is a source of slowness. It is true that the DOM is not as fast an immediate-mode drawing API. It is not designed to be. They're different things. But the DOM has all these different things that if we compare it an immediate-drawing API we're not taking into account. The first one is that the DOM offers accessibility bit in. You can an know date the DOM and you're going to get screenreader support for basically free. Other platforms have ways of doing this, but the DOM's way of doing this is incredibly easy. You almost don't need to know how it works if you're writing semantic mark-up. If I'm using Twitter on windows and I want to add emoji support to Twitter, I install a browser extension and I can type in an emijo. Twitter don't know about this. They don't care about this. This is one of the values of the DOM, this substrate that is extensible without our knowledge. That's the same thing saying the DOM is framework agnostic. You can write two widgets in two different frameworks, and as long as they're rendering elements, you can append them to a common parent and they don't have to know or care about each other. Preact at its core is essentially a DOM renderer. On him going to share a couple of lessons I've learned since writing a DOM library. The first is use text nodes for text. This sounds stupid, I'm aware of that!  It's surprising how often you will see this either miss construed or misinterpreted. The DOM has an API for working with text that we often ignore. You concede an initial value, insert these, move them around like any other elements.  They don't inherit from element, they inherit from node but and so that you can set its text.  I don't remember. So, this is a perch mark showing that that is the case. We've got text content, and then text.node value just for an a-topic update. No value is clearly faster here. If you do a library that does text update, you would be crazy to use the first one. It is not unjustified in its slowness, it is removing elements that don't belong there. It does a lot of work but in a lot of cases, we're using text content in a situation where we don't need any of that work. So the next DOM lesson - I think that's an ArrowSmith joke - is just avoid getters entirely. Don't use them. Don't rely on them, or, when you are using them, be incredibly cognizant of the fact that they are. Here is a snippet from Chrome dev tools sake if we look at the node-type property of a next node, it should be the integer three which is the corresponding type. It's undefined on the text prototype which is counterintuitive. This is a huge red flag that I have highlighted as a red flag. The idea is everything is going to have this inheritance from node. They implemented it in one place. I'm sure there's some reason for this. This is a red flag for anybody writing a DOM library. This means that, when we write code that relies on node type, by drought, this is not going to run as fast as it could. Instead, we can use duck-typing to improve the performance of this. They have split text. Split text is much faster because it is not a getter. All we're doing is checking the property exists. I know Benedict would tell you this if I didn't. You can check to see the if the property exists. The problem with the second check is that it will check if it is existing and truthy, which is two operations where the second one we don't care about. This is a performance test showing that that is true. You can see here we've got instance of a couple of different variants there, instance of is not great because if you have after text node from a different document, it is a different parent class so it is kind of useless. The getters are relatively slow but the property accesses are surprisingly fast. This is how to Preact reacts to text node. Live load lists. Please try not to use these. They were a good-intentioned thing but they end up hurt performance. Here is an example of trying to clear out children of a parent element. You do a backwards loop over what appears to be an array because the indexes are going to be all screwy if you loop forwards because it is a live node list, they change as you mutate. But we're removing the children as we go through the list. Eventually, we will get back to none and we're good. The second one is much faster though because we are accessing one property at each point in the loop. We never have to go back and ask for child nodes again. We never have to access an offset in essentially an array. We're just using references. So here's a performance benchmark. One thing to remember with the benchmark is that a lot is set up overhead, I can benchmark the elements, and in the last case, the last child is always faster. I've been doing performance optimisation for quite a while. Benchmarks were run a loop five million times, check the date before and after, and cross your fingers. It is still the way stuff works. Chrome dev tools has advanced this. It is having a tiny copy of Sam in your browser and you can ask him to run performance tests. It is far and away the best tab in chrome dev tools to use in a slide!  One metric is the total time to render a tree of components. The value here is that you can see it visually. It is 3.18 milliseconds to render this but I can see it is consistently around that same value. It's good to be able to visualise that. The next tool is from a V8 engineer, and this actually touches on what Lin was talking about DOps and compilers. I can slip over this slide. It will show you visually when your code is going to de-opt. You run this with flags, and, if you want to know the flags and figure out how to run Chrome to get the flags with the tracing output, and go to the - that only works on Mac iOS for now. This is unsurprisingly the first app built Preact in September of 2015 and it hasn't been updated since then. There are a whole bunch of UX flaws. The goals of the bench is to give you a simple user interface on top of Babble and benchmark.js. You can do comparisons of various different implementations of a given function and literally just figure out which runs fastest. You can do it in a bunch of different browsers, check it as new Chrome diseases come out, and suddenly the benchmarks have inverted. It is useful to know, that way we're not doing performance optimisations and a year down the line, suffering because of them. The first lesson is be explicit. Don't task something to a Boolean if you have no reason to do that. So, in this case, we're checking what looks like the existence of props on an object, but really what we are doing is checking that it exists and that it isn't zero, an empty string, null, false. The next one is explicit checking to see if props are defined. Reading this as a programmer, it makes more sense and faster for V8 to be able to run. The next lesson is inline helper. Functions can be way too develop and JavaScript developers because this is a language that will let you do anything you want, this is really, really common. So this is a hook function that was in Preact. It does way too much stuff. It gets deopted immediately and the solution is to rewrite it as an inline functions call. Here is a quote by me!  But basically the idea is if you can avoid making a function call, avoid it. Sometimes, there's just no reason to do certain things. In the case of Preact, it would be like if you're dipping a virtual DOM tree and there is one contextual child, it will update the diff and then bail. Probably the most significant performance that has ever gone into Preact is that particular early return. So the point of all this is to say please make decisions based on actual data. Find a way to get data, whether it is a benchmarking tool, dev tools or any of these solutions. Collect better data through benchmarks and performance tests and then act on that by setting better goals. If I can leave you with a message, it is remember we have a shared goal making the web faster and more accessible. That's how we provide better experiences to frustrated users of the mobile web. It is how we're going to make the mobile web the platform of choice in the next three and a half years - I suspect that number - and then finally, it is how we get the next billion people online so they can benefit from the access to the same resources that we have had opportunities from. So that's my talk. [Applause].