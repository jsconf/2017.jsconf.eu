Nidin Vinayakan - Shared Memory & Parallel Programming with JavaScript 
>> So we just had a panel discussion on the TV39 which is writing the living standard of JavaScript, and one of the features that came out in the most recent spec was shared array buffers and shared memory. Please give a warm welcome to Nidin. [Applause].  
NIDIN:  Good evening, everyone. My name is Nidin Vinayakan. I'm from India. I'm mainly working on web technologies. Right now, you can find me on Twitter - maybe some of you know me, like this thumbnail.  I'm on Twitter. Right now, I'm working in a start-up company and we are trying to sell cars online, so you can buy cars online with a mouse click. If you don't like it, you can return it - yes. So you will know we have web workers, and it is enabled mull think threading at some extent, so, for example, if you have a memory in worker 1, and you have a memory in worker 2, if you want to serve this data, you can serve one bit by one using force - that's not the efficient way.  
          Another thing you can actually post is the memory to the worker 2, then it will clone, which actually copies the memory 1, and the memory block and creates a new memory, so, in the memory, it will take like two blocks of memory. Another thing is you can transfer this memory. The problem is that it is fast, there is no cloning, just transfer the memory. The problem it is de-attached from the CPU1 and no more axis. If you have an object which is crazy, it will be serialised and be serialised during this clone which is expensive for a real time shared memory - for the part of the programming, this is not an option.  Yes, there's good news. We are now at ES18, and there is a new feature - shared memory and Atomics by Lars T Hansen, so we have a new RPA or buffer - so, when you create a buffer it will allocate a memory block in the RAM and you can work with it, but the problem is you cannot share it. To share the RA buffer is the same but the underlying memories shared between all the workers. So the API is similar. It's the already shared between workers, so just post it, so will transfer the reference to this memory to all the workers, and all the workers can act from the same memory, read and write to this memory. Let's do some executions. We're actually counting 200 million numbers. How long does this program take?  Time for 200 million iterations. How can we make it faster?  Run iterations parallel. It is hypothetical how we can run it in two CPUs. We are expecting still above two. Actually, it is slower. Why?  Because we forgot this little thing because the CPU has its own cache and there is the L1 cache and if you want to access the memory to copy to L1 cache, so if you're doing a computation only in one thread it is faster because all the memories are immediately available in the L1 cache so the accessing is really fast. If you want to share operation between workers, you need to go through these things. Also there's another set of problems, because it can cause data data arrays, then relevant to CPU1, maybe CPU 2 has already fetched this data and implemented by one and it writes to the memory. But CPU1 doesn't know it. What will CPU 1 do?  Increment by one and write to the memory. At the end, you will get only one, but should get two, because there is no co-ordination between CPU 1 and CPU2. For that, we new a new set of APIs, called Atomics.  We can actually look at one particular line of memory access. During the log, CPU cannot reading or write that operation - CP U2. We lock the memory, we load it and we added and stored it to this memory and released the log.  Then CP U2 can implement and read and write to this memory using log - or without log. If it is not logging, the same problem might happen. The new API atomics, only available in ... so we cannot do ... use when you do this Atomic add in 3201, at the end, you get the expected result. Let's do two small ... . This is our symbol program, counting, so threats count in single thread. So take around 400 milliseconds to count these 200 million numbers. Then count in parallel. It takes longer. The - it it's not 200 million. If you run it again, it is different. Each time you run, it is different data because there's a data race. It is not coordinated, they will write depending on the hardware, depending on the cycle, how the CPU is busy. The CPU will schedule this in sections independently. We can run with Atomics. It is quite slow because it is doing its kind of serial because it is logging the memory and adding it, so during that lock state, the CPU1 cannot add it, so, yes. You will get the expected result, but it takes like 13 seconds. That's ridiculous. It is actually very, very slower than single thread.  That problem is is I will show the code. Formerly accessing this Atomics add any time, like 200 million times using Atomics.add because it is logging 200 million times. That's very, very slow. Like the simple optimisation. This is very fast. You also get the expected result. Then, when you run this single trip again, so, when you run a single trip again, it is actually much faster now because it is actually V8 is optimising, it is 50 per cent faster now and then you come back with these 400. So, in the optimised version, what I'm doing is, because accessing memory is expensive. When you distribute some data, so, I'm actually using a local count, then I'm actually counting locally, because I have eight threads, and I tribute this 200 million counting to the eight threads, so 200 million divided by eight. So, in this loop, it only uses the registers, like this log. The count is a variable so it will store it to the CPU register, then during in foreloop, the access and implementing it's very fast. After counting the local news, only using one Atomic.add, that's why it's faster. If you're really doing any real power programming, yes, the Atomics is very good, but you should not do it if you don't use it, or if you don't really need to synchronise the data. So I was working on a GGS render engine so, using web gel it is very hard to mimic the physics of light. It is very hard to mimic these global illumination, and realistic shadows, and soft shadows, so I develop a CPU version of red racing engine using shared memory. This is a standard testing model, stand for dragon. The Stanford dragon. I need to built the tree, £is building a parallel, 2.8 milliseconds and I can start rat racing. Okay. It is actually rat racing in real time in JavaScript. So, it is actually splitting the scene into, or imagine to eight styles, and it is rat racing eight styles at a time. For JavaScript, that is quite fast in this case. I can actually turn around. When you toggle the versions, you can see the shadows are very nice, and also there is colour bleeding here, the red one, and also green colours here, which is very hard to mimic using web gel which is a rasterising web - it is flattening all the 3D data into 2D. In this rat racing, we have a shooting race into the scene, and I'm intersecting into the 3D model and depending on the normality of the flags and try to find out the accurate colour. So I left it running longer, we would get a realistic image. So there is another three.js demo scene, it is opt data with materials and textures. So, this is vanilla JavaScript. And it was really tough developing something like this, because I have only shared memories, and it is a linear memory. Like there is no structure, or I cannot copy objects, so I need to put everything into shared memory. It is very linear, so I need to write objects into this memory, so doing it manually is like, it is not sailable, and it's not - if there is a bug, it is hard to detect. I have an object, and the object has some properties. I cannot write or read directly to the shared memory, because it is a JavaScript object. What I need to do is serialise the kind of end-code this object into normal binaries like object ID and property value, so writing directly into this memory. It is really not optimal, because for a little better project, it is not an option. So I need something like okay an object. I'm using Typescript now so I have an object, and I need to do this automatically. What is the solution?  I didn't find any good solution, so I created a compiler, called Turbo - when we create a task in the script, it will be a shared memory and then a pointer, so I can work with the pointer.  So yeah, the turbo script looked like this. So, for example, it is a vector, and the vector has x, y, z, so, for you, it looks like normal typescript. The only difference is you need to delete this allocated memory manually, like C++ so it can create this object. Here I'm creating this new vector, and I'm exporting this function. When you export a function, it will actually export, it will create a module and export this specific function from the JavaScript. It will create a new vector and return a pointer, and later, you create two vectors, and you use this function to add it. It will add a new vector, then if you are done with that vector, destroy this vector, otherwise it will leak the memory, and he will die!  Turbo script is a subset of typescript. It can come back to normal JavaScript. In WebAssembly, it is not really useful, because WebAssembly doesn't have shared memory, so WebAssembly 2 releases, I can really test the performance. I can compile the same code to two or three different targets, normal like JavaScript, and have some - it will be a really nice tool to compare how these three things are performing. It's still a work in programme West, so I'm trying to implement it. So a typescript like JavaScript, some combinations, in the future it is not supporting WebAssembly or in order to support all the JavaScript features, I need to re-implement all the JavaScript original machine inside WebAssembly. That's not doable for now. I can actually compile some functions into the WebAssembly and other functions into the normal JavaScript and like connect together, for example, DOM access things I can come back to the normal JavaScript, so it's easy. Maybe some hard functions, number-crunching like that. Then, you can utilise this WebAssembly, because normally when I talk about normal JavaScript developers, they're not interested in WebAssembly because right now, own C++ can compare to WebAssembly and when you talk about WebAssembly they don't know what is it, and they can't really use it. You can still write an expression in text format which is really not scaleable or it is very low. It is meant for debugging and no-one is writing a real big project in it. So this is also a good tool for normal JavaScript developers because its syntax is similar. So merge mark, and I'm useful normal JavaScript. I need to - I mention I need to create a vector for race racing which is KD3. To build it, it will take a little bit more time because it's actually splitting the 3D space into different small, small pieces.  Normally, without TurboScript, I build once in the main thread or copy these objects through both messages and then to actually clone and the performance was low, and it will take more than one minute just to initialise this Standford Dragon. Right now, I'm using turbo script. It will compile and at the end get written a pointer to this 3D the workers has immediately access to the KD3. It can stop rat-racing. I will show you some syntax. There is an RAX sample in turbo script. You can also export the class, so you export something like this. We will open it so it is easy to read. So it will declare like this, like data new and data search. From the JavaScript, you can use data new so it will create a new instance of that data class. And one problem is I'm working on it, I'm working on a wrapper for these turbo classes so from JavaScript, you can access like instance.properties. Right now, you can't access properties, but you can access methods, so you can abscess set method, or if you want to access a V1, so you need to, like, catch V1, you need to do something like this to access the property, because when you - you're getting a pointer, because it is in 32 number, you cannot dot it because it is a pointer. Up need to do like this. But I'm planning like a wrapper function around the turbo object so it can actually export automatically a JavaScript wrapper, then you can use normal things, but I don't know how fast because it will create an object, maybe for a bigger objects like if you have a queue, for example, a big scene of millions of triangles, you can create wrapper objects for the main route objects like a scene, so from the scene, you can access a lot of the properties. That will be useful. Yes. And also, I'm using this wasm tool, I can format this text format and debug what is going on in my assembly. It is easy for debugging. Also, I have a log, so to limit all the data like what bytes are retaining, so it is easy for debugging, everything in the log file, but it is formatted.  And yes, any more questions right now?  No. Okay. Yes, that's it. [Applause].  
>> Thank you so much, Nidin, for the excellent talk. We have a break scheduled now. 15 minutes, and then the last two talks of the conference. We also have our last community event. If you want to go and check out another { live : JS } performance and a meet-and-greet session with the representatives. So, one more talk there, and then two more talks here. Enjoy! 