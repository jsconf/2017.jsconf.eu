"Making the Jump: How Desktop-Era Frameworks Can Thrive on Mobile"
Tom Dale

>>  Thank you so much, Ashley.  That was really amazing.  And before you go, I took a photo of this.  So this store, in New York, that sells karate hip hop action/drama headphones and luxury items.  Have you ever been in this store?
    ASHLEY:  No, I just stood outside.  I'll come back and let you know.
>>  Awesome.  Thank you, Ashley.  Okay.  We have two more talks left today.  We're next, welcoming to the stage Tom Dale.  He is from New York City, of course.  He's a JavaScript thinkfluencer.  So this is going to be a think-piece talk, I think.  He's wearing a suit today because he's a contrarian, apparently and I think he wants everyone to take a photo with him in the photobooth truck, which every time I walk past, there's not enough people in there.  So I hope you've all had a turn in the photobooth truck.
    So Tom's also wearing a suit because he wants you to join his professional network.  He works at LinkedIn.
    
    [ Laughter ]
    
    He looks very professional.  So he's going to give a talk today on how desktop-era frameworks can thrive on mobile.  I'm a little bit sad that he's actually in the completing the look with the bowtie and didn't go all the way.  Thank you, Tom.
    TOM:  Thank you very much.  Thank you all for having me here today.  It really is an honor to be here.  My name is Tom.  JSConf is a very special conference for me.  I was here about four years ago, which is when I met Zahra, who you saw earlier.  Jed introduced us on the plane and then two years after that, I proposed and we're getting married in August.  So, for me, in particular, JSConf EU has been particularly life-changing and hopefully many of you can develop specially similar relationships, professionally, I mean.
    So today I want to talk to you about smartphones.  And in particular, I want to talk about how tools that grew up on the desktop like Ember, Angular, and React, can make the jump to the mobile future.  And I think when people think about adapting apps to phones, they often focus on the more obvious differences.  So, for example, things like the difference in screen size.  The difference in CPU performance, the different input devices, touch versus a mouse.  But I think where the device is used is important, too.  So, for example, a user on a mobile phone is more likely to be distracted than someone who might be focused on a task at their desk.  Or they might have a more intermittent network connection than having a broadband Wi-Fi router in their apartment.  
    So when phones can be used anywhere, there are very few assumptions we can make, whether that's how attentive the user is, or whenever they have that Internet connection at all.  So I started working at LinkedIn in December and one thing about working at a site like LinkedIn is it reminds me of how truly global the web can be.  In many cases, adapting to smartphones really means adapting to new users.  For many people, their first computer is a smartphone.  That means millions, maybe even billions of people are going to be participating online without ever owning a desktop computer.  And the more global your app is, the more combinations of devices and networks you'll have to deal with.  CPU power can range from a feature phone to a low-end smartphone, to the latest iPhone, up to a beefy desktop computer.  And network connectivity can range from GPRS, to gigabit fiber, to nothing at all, just ride the subway in New York.
    So the problem with this is that without careful design, it's easy to optimize for one combination at the cost of another.  So let's take a look at two different users.  Let's imagine these two scenarios.  So user A has a very low-end smartphone.  It has a CPU has easily overheats and starts throttling itself, and it would be borderline useless except for the fact that it's not full.  It Opera Mini which heavily processes requests before the user gets it.  User B has a phone with a CPU that might rival a desktop computer and they've got plenty of fast storage.  But the problem is this person is traveling without any cellular data.  So while they have sometimes access to broadband Internet, it's only when they're within range of a Wi-Fi network that it's useful.  For user A, anything that's JavaScript is probably not going to be helpful at all.  Even if they stopped using Opera's proxy, with the slow CPU meaning that getting the JavaScript means long load times.  It's about keeping the file size as small as possible.  For user B, we want our web app to work more like a native application.  In fact, we'd be willing to spend more time up front to load the entire app and as much data possible on the phone.  That's basically what the App Store does, right?  
    So that meant that he could use it when we were away from the Wi-Fi connection that we were relying on, and this user probably has higher expectations on this app, as well.  It would be worth sending a bit more code to get 60 frames per second animation and buttery smooth scrolling if their device can handle it.  So best experience on a slow device is often radical different for the best experience on a faster device.  And the more we try to take advantage of higher end phones, the worst we've made the experience for the majority of the world.  So what do you all think is the solution to this problem?  I'll give you a hint, the initials are, "PE."  Any guesses?  That's right!  Panic and evacuate the building!  Very good.  The answer is supposed to be progressive enhancement but one thing that's implicit, at least in all the advice that I've received about progressive enhancement is that you're supposed to do it yourself.  
    It almost always means rendering on the server, and denying yourself the temptation of using too much JavaScript.  And browsers in the last ten years, let's say, have advanced at a remarkable rate.  To me, it feels like the web has never had more momentum add feature after feature.
    And, yet, despite all this incredible innovation from index.db to web workers, it doesn't actually feel to me the experience of using web apps day to day has improved that much in the last three or four years.  So why is it that these radical improvements in the browser don't seem to be translating into radically improved web applications?  And I would argue the reason for this is the cost of code is too damn high.  Taking advantage of all of those new features in the browser requires a lot of code.  Native apps that work offline with beautiful user experiences with hundreds of megabytes and that's not even factoring in the SDK that ships with the operating system on the phone.  On the web, just parsing and downloading JavaScript can be enough to turn phones janky, and when you start bundling your JavaScript into a file, every byte starts to count.  And in turn, misaligned libraries have to compete on file size rather than robustness.  So how do they achieve these improbably small file sizes?  Often it's by persuading you that the old thing is unnecessarily complex, the cardinal sin in JavaScript but they've seen through the BS and built something simple and that leads to us the JavaScript simplicity fetish.  
    It is this emphasis on file size that leaves the JavaScript community to this fetish, when file size relies on simplicity and speed relies on file size, and speed is paramount on the web, we have to pretend that simple tools are the best tools because what other choice do we have?  The only way to run an app that run well on slower phones and networks is write less JavaScript, too often, handling edge cases and higher levels of abstraction.  We eventually start to collapse under our own page weight.  That's rational.  We've seen this play out several times now.  The more sophistication equals more code, equals slower load times.  So the time period from 2011 to that we have could be roughly broken up into the Backbone era, and the Angular era and the Ember era.  This trio of eras can sometimes also been known as the Ember era.  Now it's easy to become enamored with the complexity of a tool.  And let's go back to 2011 and the most cutting-edge tool that people were using was Backbone.  And people were saying, I love it, I can clone it from GitHub, and read the source code within a day.  But whenever my model changes, everything breaks.  I heard about this new library called Angular.  Two-way binding is so simple.  You just change it, and it changes it on the screen.  But a year later, it's like my app contains a controller with 3 million lines of code.  But don't worry, React is amazing and so simple because it's just the "V" in MVC and a year or two into building your app you realize it's 7 megabytes and takes two years to build.
    So Don Norman what you may know as the author of design of Everyday things, wrote this:  The numerous defeats in security measures prompts my slogan, the more secure you try to make something, the less secure it becomes.  Why?  Because when security gets in the way, sensible and well meaning people develop hacks.  Like password rules are too annoying, people just write the password and put it on their desk.  So I would like to offer a different corollary because when simplicity gets in the way, sensible, and well meaning people develop hacks to defeat the simplicity.  So how do we break out of this local maxima.  How do we write one app that can scale up and down across these different devices and performance characteristics?  Well, I think we can learn from native developers because they've had to tackle a similar problem.  So these are different CPU architectures and different CPU architectures have different instruction sets.  If you write some code for x86 and you want to run on ARM, they're totally different.  If this is how software was written, there probably wouldn't be that much cross-platform code and introducing new CPU architectures would be borderline impossible.  We've figured out, a long time ago, that a compiler can take a higher level program and get it to run across all of these architectures and if a new architecture comes along, you just have to update the compiler and not rewrite every app in existence.  So here's an existence of plying an LVM, and taking C code and taking an architecture that didn't exist in the '70s, which was WebAssembly.  And best of all, the compiler makes our code not only run on all these architectures, but it can be used to opt my our code differently for particular characteristics of that platform.  So, for example, here's a paper about optimizing GCC, where they said, finally, a new target-independent feature could be implemented to take into account certain specific features of the target.  For example, to make advantage of the Intel Itanium, it's dating the paper, we've implemented the support in the instruction scheduler.  And this is something that Ashley said in her talk when I totally agree with, if there's one thing that I want you to take away from this talk is that modern web toolkits are transforming away into something like a compiler where instead of compiling your into native code, it's going into a highly optimized version.  
    And I think this would mean taking an app and delivering the most optimized version to your device.  This could be like delivering new ES6 builds, or transpiled ES5 builds and/or transitioning from server-side rendering and client-side rendering based on the network speed detected.  But most importantly, these tools can decouple the file size.  If we can shift the complexity to our build tools rather than being these monolithic runtime libraries, we can reduce this pressure to be simple.  So unfortunately I don't have that much time for you today but I wanted to highlight three things that the teams behind React, Ember, and Angular are working on, in order to demonstrate that these things are becoming mobile on compilers.  And so the first thing that I want to talk about is React, and also Prepack which is an open source tool for optimizing JavaScript validation.  Prepack was released while on the plane on my plane to Berlin.  But it was definitely true, the Prepack website uses quite a bit of computer science terminology to explain what it's doing.  It's genuinely exciting and novel but you don't need to understand heap serialization to understand why Prepack is cool for optimizing web apps.  So to understand why Prepack is school, let's see how something like a Rollup works.  It starts by analyzing which file it imports.  For each of those files.  Once it's analyzed the entire graph, it builds a new JavaScript file that includes just the modules that were actually used.  And once -- the final output is on the right.  Everything we've imported is inlined right into that file and modules that aren't imported with excluded.  This gives us smaller files by eliminating the files that we don't actually use.  But it's not just modules.  Rollup can do other things if it sees things that we didn't use.  Notice that we've added a new class to animal.js, that's because Rollup is smart enough not to include the feral animal class static analysis just means figuring out things about how a program will run without actually having to run it.  
    Rollup isn't running your code, it's just scanning to figure out what gets imported and what gets exported.  And modules don't work inside of conditionals so it knows with 100% certainty.  That's roll-up.  Let's take a look at V8.  Specifically, I mean, any JavaScript virtual mean but we'll look at V8 specifically.  V8 can't help you with file size.  By definition, it has to have downloaded the JavaScript file already but it can help with making your code faster.  So this is a high level view of the architecture, V8 and there's three major components.  I think there are V8 people here so please forgive me if I get anything wrong.  So the parser which turns your JavaScript into the instruction, and then the interpreter ignition which evaluates your JavaScript and then the Turbofan which turns your JavaScript into something more time to generate but it can run at truly ridiculous speeds.  And as your program executes, it keeps track of how they're run and based on this information it will ask Turbo fan to make optimized versions of your that it thinks it will make it run faster.  Not running it JavaScript probably would be considered a bug because because it relies on how the program actually runs, you have to use the unoptimized code for a while until V8 can make discussions and that that means that every user pays that cost even though the ended result is more or less the same for everyone.  This is the tension that we have with static analysis and dynamic analysis you pay the cost once and all your users benefit but static analysis can only get you so far because you can only perform optimizations you're 100% sure about.  If the module might get use, Rollup can't use it because if it guesses wrong, the app breaks.  Information about the runtime analysis can collect information about how the program actually runs so they don't have to guess but requiring the program to run in order to optimize it is a bit of a catch-22.  
    And then that's the opposite of what we want on the web where things typically want to feel instant.  So that's what makes Prepack so cool.  What it does is actually run you are why code.  Similar to V8, it's an actual JavaScript virtual machine but rather than running apps, though, it runs at build time and it reverse-engineers a version that's faster for your JavaScript to execute than the first time.  So this whole thing is a talk into itself and I'm still digesting how it works internally but let's just take a quick look at how it works.  I know I showed an example also put so imagine we're writing a library that prints the data of JSConf EU, so publish -- so if we run this through Prepack, you can see that it's turned it you will into a string literal.  But instead of every user having to allocate a data object, parse a date string, initialize a date object, and generating an ISO-601 string, and this is a contrived example.  And each of these small stakes may start to really add up in codebase the sizes Ember, Angular, or React.  And I'm hearing a rumor that Jason Miller is going to release something called pre-Prepack, that does everything that Prepack does in only 600 bytes of code.  So let's look at Angular, it's on Typescript that's just a subset of JavaScript, and those types makes refactoring much easier as your project grows, but to me, the most exciting and underappreciated reasons for is the potential for using that type information to more aggressively minify your builds.
    So in this examiner we've renamed this variable, fruit, to be called o and because these variables inside this go nowhere outside of this function can access it, so whatever we call inside it is arbitrary.  Let's change this example a little bit and instead of making fruit a string, we make it an object with name property and even though the variable fruit gets mangled, the property name stays the same.  Now uglify.js has a property -- renaming properties can break things.  In fact, the readme of uglify says, note, this will probably break your code, which doesn't instill a lot of confidence.
    So, for example, if we have an element and we change its onclick property, if that gets renamed to something else, then this is just going to fail silently.  That click handler that we thought we were handling just doesn't exist anymore.  And similarly if we look at a component, this is a React component, for example, there are parts of this object that are private and then there are other things that are public, meaning things like a library or a platform rely on it.  Really what we want to do is mangle the private stuff but make sure that the library can keep using the public stuff and there's good news.  Google's closure compiler already supports this advanced minification for JavaScript.  Unfortunately, it requires an notating types with js style comments and -- approximately seven people on the planet outside of Google have actually gotten this to work.  Now the Angular team has released an amazing tool called Sickle that translates a type script project who the bizarre format that compiler, and because Angular is written in Typescript two, closure can do some advanced minification.  The idea it more accessible to broader world, and integrating into this NGCLI by default I think would make a huge impact into basically every Angular app.  So lastly I just want to talk about Ember, and specifically Glimmer, which is the rendering engineer in Ember, and also the library that we extracted to be used standalone outside of Ember a month or two ago.  So now in thinking about the rendering engine is there's three things to consider.  How fast does it take to load.  But then once you have something within reasonable bound of time, you want to look at how long does it take to render a component for the first time when you have to create brand-new DOM elements.  And third, how long does it take to rerender a component updating its existing DOM elements instead of replacing them when the data backing that component changes.  Okay.  So who remembers this app?  Yeah, the db bain of my existence.  This is Ryan Florence's dbmon demo app, which heights the differential app and everyone was competing to have the fastest score on it.  And it's all anyone ever focused on but then more recently the community's focus has been on initial load times as people have grappled with how we deal with these low-end phones and slow networks.  And the tricky thing is trying to find the sweet spot that balances between optimizing the first render and optimizing rerenders.  Fundamentally, this is an issue of bookkeeping.  The more bookkeeping you do in the initial render to make subsequent renders faster, the longer that that render is going to take.  So consider a render using Bootstrap markup on VirtualJS.  That makes a lot of sense.  This is basically just instructions for how to create the real DOM but if a single part changes we have to run the entire render function again and that's quite a few allocations of virtual DOM objects, not to mention that once you have them, you have to then do this diffing step to kind of reverse-engineer what changed between last render and this render.  So the performance cost is probably negligible but in larger apps you can see how it starts to add up.  And in Glimmer we use Handlebar templates and success how do we compile them into.  Come Ember's history, this answer changed three times, first it was strings and then DOM, and into with glimmer, byte code.  [Inaudible], and a virtual machine that runs that compiled code.  So in Glimmer we compiled this template into a JSON object that looks like this.  Now, this probably doesn't book meaningful to you but what this object represents is a series of op codes, or instructions how to build -- into integers, string literals, so parsing is fast and memory consumption is much lower.  When we landed Glimmer in ember 2.10, many apps found their template sizes dropped from 75-85% which is significant.  Now if you look closely as 30 but we use integers to represent instructions for compactness and for certainly V8 operations but if we were to operate it in a hypothetical linearized set of instructions so when we take those instructions and we execute them on top of the Glimmer VM, it looks a little bit something like this.  We have an instruction pointer that just goes through this sequence and so this is going to build an li element and then we're going to move on and run the static add p code and that's going to set the attribute and then we're going to add another static attribute which is the active class and then that's the whole template and then we're going to flush the element which tells Glimmer that this is something to go to put in the DOM.  Now this isn't this is an easy example because this is static.  But what I want mentioned yet is the Glimmer VM is made up of two VMs.  One is constructing DOM initial on initial render and the other is for updates.  So this is the program for initially rendering this template.  Now we could run this again for updates, right?  But there's a lot of static content in here and spending time executing op codes for static content we know hasn't changed isn't a good use of time.  In fact, we just want to focus on this dynamic part and the way that we optimize is through a technique called partial evaluation.  And partial evaluation simply means to generate an optimize program by evaluating an initial program.  And we use this technique to generate it automatically.  So initially as the initial program is being run, we're generating a more optimized version when the data changes.  Essentially these operations in the initial rendering program are responsible for generating their own subsequent code.  So the way this works is that we're going to go through our op code for the initial render as before that's going to open a new div and we're going to set some status traits on this, so in addition to appending that text content to the DOM, we're also going to generate an updating code which is to update that content and it's going to include a reference to what value what you were putting in it as well as a reference to the text value that you created.  Now when that data value behind that component updates instead of going that whole process again, all we have to do is evaluate our highly optimized updating program which is just going to take the old data out of the DOM and update it.  And you can think of it doing as something -- it's a little more complex than this -- but you can think of why it ends up being fast because it needs -- notice that basically all the rendering library benchmarks only tested pure dynamic content but it's not representative of apps in the world.  So we created benchmarks that had a mix of both static and dynamic content and what we noticed was that initial I'm not putting the other names here because it's not a competition.  I think I wanted to show that we're basically in the same ballpark.  However, when we moved to updating other performance what we noticed was that it seems like having this optimized bytecode makes a big difference, which can make a big difference, especially on lower end phones.  
    So today we've talked about three popular frameworks and things that they're doing to improve performance on mobile devices.  It's -- there's an exciting trend towards more sophisticated tools that can analyze your app, perform optimizations that can be time consuming, practically impossible to do by hand.  Best of all code that clearly conveys intent to other humans has a funny way of conveying the intent for optimizing compilers, too, p so in that we have be the idea that build tools are -- sophisticated build process of a modern web app.  This morning, Addy showed a bunch of techniques for measuring and improving the performance for your web apps.  That's awesome information but not everyone can be Addy standing over them and reminding them to take care of performance and spend time learning about these techniques so I'm incredibly excited about a community that can help democratize and commoditize that performance know-how.  So the best approach of approaching frameworks and compilers isn't that they just help today's by reducing the cost of code, we can build better apps that don't collapse under the weight of their own complexity.
    This is a very exciting time to be a JavaScript developer.  I for one am really looking forward to web apps that load instantly, work offline, feel great to use for everyone.  Thank you.